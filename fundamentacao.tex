\chapter{Background Theory}\label{chp:fundament}
\vspace{-1.5cm}
\noindent\rule{\columnwidth}{1.2mm}

%\begin{figure}[h!]
%\centering
%\caption{Regiões sensoriais e seus segmentos medulares}
%\includegraphics[width=0.91\linewidth]{Imagens/corpo.pdf}
%\label{fig:corpo}
%
%\small Fonte: \citeonline{ASIA}
%\end{figure}

\section{ROS}

There are many problems when developing robot applications, especially because the complexity of those systems. ROS is not an operating system per say, but a framework that allows coders to readily develop and test solutions with modularity and code reusability in mind. It was built in a agnostic package system that allows integration from many packages available from the ROS Open-source community, many of them implementing support libraries and proof-of-concept algorithms, as well as core infrastructure.

The main aspects of ROS are \cite{quigley2009ros}:

\begin{itemize}
\item Peer-to-peer: even though the ROS framework relies on a master or namespace as a lookup mechanism, the communication is established between peers, avoiding unnecessary routing through slow links when the recipient is on the subnet.
\item Tools-based: instead of building an intricate framework, ROS instead relies on a set of tools written to perform task, including various tools for compilation, tap data stream, data plotting, configuration, documentation generation, etc.
\item Multi-lingual: since communication between nodes relies only on XML-RPC, they can be implemented in any language, either by explicitly writing the full library that interacts with roscore or building a wrapper for the ROS C++ library.
\item Thin: many robots implementations have parts of code that could be reused in other project if they weren't so entangled with all existing code. ROS proposes an architecture where code is separated into packages that holds no dependency on ROS. All packages can be built individually using one CMake each, different from the traditional software paradigm where one CMake file builds the entire project.
\item Free and Open-source: ROS source code is publicly available and released under the BSD License, allowing copy and redistribution of the source code, including for commercial purposes.
\end{itemize}

\subsection{Packages}

In order to better build robotic system, ROS adopts a packaged architecture, making every subsystem of the application run separately from all the others. Every package can contain new nodes, libraries, configuration or even datasets.

The main advantage of this approach is making code more organized in it's own subsystems and sub-subsystems, that specialize in doing an specific task very well so that other packages can use this functionality. Packages can also be written independently of language, as long as it's supported by ROS.

\subsection{Topics}

In order for nodes to communicate, they interact publishing messages in topics. Topics are anonymous buses where each node can publish messages following the message type standards. Each node can then subscribe to topics that are relevant to them and act upon data captured on the topic. Message can even be recorded and played back to support applications that will need the information later in time.

% TODO: refazer imagem para evitar citação
\begin{figure}[!ht]
\centering
\caption{Topic initialization.}
\includegraphics[width=0.5\linewidth]{rostopic}
\label{fig:rostopic}
%\small{Source: ROSWiki.}
\end{figure}

The topics are implemented in a XML-RPC protocol, using a master node in order to provide name resolution. In order for a publisher connect to a subscriber, they do the following steps:

\begin{enumerate}
\item Subscriber register with the master the topics it will be listening to.
\item Publisher register with the master the topics it will be publishing to.
\item Master informs subscriber of a new publisher.
\item Subscriber request a topic connection with the Publisher and negotiates a transport protocol.
\item Subscriber connects using the selected protocol.
\end{enumerate}

It's important to notice that, once the connection is established, the communication is maintained peer-to-peer, using former protocols like TCPROS, built over TCP/IP, and UDPROS, built over UDP. This not only provides faster communication inside the same network by not requiring the messages to be relayed through the master, but also enables communication through available networks of the Internet protocol suite, including 802.11X wireless transmission.

The topic configuration also enables true agnostic packages, since the communication between them will be done using a standardized communication medium, loosely coupling the packages and making them easily maintainable. Debugging can also be done using command-line tools that wiretap this medium and display the information exchanged by nodes.

\subsection{Services}

The topic communication can be very useful in many-to-many communication, but lacks support when sending messages or commands that require a response. When a reply is needed, it is a better practice to use services instead of topics. This is especially true for tasks that need a lot of computing power but only need to be executed once in a while, so instead of calculating it in every iteration, the service can be run just when requested and return data to the caller. The request is usually done in a similar way to Remote Procedure Calls (RPCs) in programming languages.

\subsection{Message types}

Since nodes need to understand each other, they talk following pre-defined message standards, and the message files themselves are packages that define the content of each message. To avoid confusion, each topic is initialized using a pre-defined message type and every node has to conform to it. Let's take a look at the message definition \texttt{geometry\_msgs/Twist.msg}, that is used in navigation to define linear and angular velocity for the joints:

\begin{lstlisting}
Vector3  linear
Vector3  angular
\end{lstlisting}

Note that the \texttt{Vector3} is not a base type message, but is another type of message defined at \texttt{geometry\_msgs/Vector3.msg}:

\begin{lstlisting}
float64 x
float64 y
float64 z
\end{lstlisting}

The variable types that cannot be expanded into other definitions are called primitive types. The primitive types are:

\begin{multicols}{3}
    \begin{itemize}
        \item \texttt{bool}
        \item \texttt{int8}
        \item \texttt{uint8}
        \item \texttt{int16}
        \item \texttt{uint16}
        \item \texttt{int32}
        \item \texttt{uint32}
        \item \texttt{int64}
        \item \texttt{uint64}
        \item \texttt{float32}
        \item \texttt{float64}
        \item \texttt{string}
        \item \texttt{time}
        \item \texttt{duration}
    \end{itemize}
\end{multicols}

Message types for services are done in a similar way, but since services support response, the message will consist of two individual messages. If we look at a standard definition found in \texttt{std\_srvs/SetBool.srv}:

\begin{lstlisting}
bool data
---
bool success
string message
\end{lstlisting}

This service is used for setting a boolean variable to \textit{true} or \textit{false}, that can be activating or deactivating an actuator. The response consists of a boolean that tells if the operation was successful and a message for better description in case of error.

\section{Simulation}

Since deploying a test robot at every code change is not possible, and ROS only provides the tools to develop a robot system solution, there is a need for offline simulation of the robots, avoiding having to test every configuration on physical hardware. In this cases, it can be helpful to set up a rigid body simulation. The normal workflow for setting up a simulation is:

% http://ode.org/slides/slide2.html
\begin{itemize}
    \item Isolating the important variables in the physical process.
    \item Model the physical system behaviour using equations.
    \item Find a method to solve the equations, given the inputs and the initial state of the system.
    \item Write a computer program that can do that simulation.
    \item Simulate and benchmark the results against the physical system.
\end{itemize}

Repeating all this five steps for every physical system will lead to the best results, but can be time consuming and difficult because all the variables involved. But since the possibilities for a robot system are often limited, physics engines or physics SDKs (source development kits) were developed to aid simulation.

While the real world has a lot of complexity, rigid body dynamics can be simplified in rigid bodies, joints, collisions, friction and springs, elements that are important to the process. The physical model can be reduced to the laws of motion, using mass, velocity, acceleration and force as simulation variables. Since the model and the solution to these problems are well known, generalized solvers can be written that compute the simulation at each instant of time, or timesteps.

Many simulators implement this generalized solvers, as well as render the 3D models to show to the user, including game engines like Unity and USARSim (based on Unreal engine), commercial solutions like Microsoft Robot Studio, Webots and MATLAB, and open-source projects like Gazebo \cite{craighead2007survey}. Since Gazebo adopts the mentality of the ROS project of being open and free, it became widely used in the community, and as a result better developed over time.

Gazebo provides ROS with a framework to simulate and benchmark the robot or even a group of robots accurately. The \prettyref{fig:gazebo-sim} shows the Gazebo simulation for a supported robot \cite{koenig2004design}.

\begin{figure}[!ht]
\centering
\includegraphics[width=.5\linewidth]{gazebo-sim}
\caption{Gazebo simulation suite.}
\label{fig:gazebo-sim}
\end{figure}

The Gazebo simulator offers:

\begin{itemize}
\item Dynamics Simulation using multiple physics engines.
\item Advanced 3D Graphics for high-quality rendering.
\item Sensors and Noise, to generate reliable sensor data, compatible with real-world sensors.
\item Robot Models, including ones from the community.
\end{itemize}

All the robot description is made using a URDF file or a SDF file. The URDF or Universal Robotic Description Format file is a XML file describing all elements of the robot. Even though it's called ``Universal'', it lacks some of the features like parallel linkages, friction, etc. To get around this issues, a new model called SDF or  Simulation Description Format was developed specifically for use in Gazebo, while the URDF was maintained for backward compatibility. Every time an URDF file is loaded it is converted by Gazebo to an SDF equivalent.

\subsection{URDF}

The URDF must start describing each link and it's inertia.  \prettyref{fig:gazebo-sim} show three links: \texttt{base\_footprint}, \texttt{wheel\_left\_link} and \texttt{wheel\_right\_link}. They are loaded from STL or Collada files included in the project. \prettyref{fig:gazebo-stl} show the STL renders for the robot shown on \prettyref{fig:gazebo-sim}.

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\linewidth]{gazebo-stl}
\caption{STL Link files.}
\label{fig:gazebo-stl}
\end{figure}

The inertia is described by it's inertial parameters, formerly the mass, center of mass and Moment of Inertia Matrix. Additionally, you specify a collision box that will aid collision testing in simulation. You can then describe the rest of the robot, including the joints that will hold links together. Two basic joints can be seen, \texttt{wheel\_left\_joint} and \texttt{wheel\_right\_joint}, the hold all three links together. Finally, the plugins will give the robot the simulation functionality, adding IMUs, Laser Scanners, Cameras, etc.

\subsection{SDF}

The SDF is an improvement of the URDF descriptor format that supports more functionality. It not only supports all the physical descriptions shown in the URDF model, but also features:

\begin{itemize}
\item World information, from lightning and gravity to complex information like magnetic fields, winds and atmosphere (temperature and pressure).
\item Scene information, like ambient, background, sky, fog and shadows.
\item Multiple robots descriptors.
\item Choosing physics engine.
\end{itemize}

\subsection{Physics Engines}

In order to simulate the physical conditions of the robot and the environment, Gazebo supports four different physics engines: ODE, Bullet, Simbody and DART. At the start of each simulation, the user can select the desired physics engine changing the startup flags for the program or configure it on the SDF file.

ODE or Open Dynamics Engine is an engine for simulating articulated rigid body structures. It features a stable integrator, meaning that numeric errors shouldn't grow out of control. Because of that, the simulator drops physical accuracy in favor of speed, stability and robustness \cite{smith2005open}.

Bullet is a Python implementation of physics simulation for robotics, games and visual effect that provides forward and inverse dynamics and kinematics, as well as built in collision detection. Bullet differentiates itself by being easy to use and provide integration to machine learning frameworks like TensorFlow \cite{coumans2018}.

Simbody is a multibody simulator focused on biomedical research. It was developed to better suit simulation scenarios where engines like ODE may not converge to correct results because of lack of fidelity in favor of real-time performance. It is used for neuromuscular, prosthetic, and biomolecular simulation, as well as design and control of humanoid robots \cite{sherman2011simbody}.

DART or Dynamic Animation and Robotics Toolkit is another rigid body simulation that distinguishes itself by accuracy and stability. The main purpose of this simulator is providing full access to internal kinematic and dynamic quantities \cite{lee2018dart}.

All four engines are used to tackle different problems. By varying the time step, for instance, the you might obtain better results in your simulation with one of the engines. Usually, when choosing the simulation engine, the relation between number of iterations, error and speed must be taken into account. Some of the simulations require better precision, while others may require real time performance. For a more in depth comparison of the four engines, see \cite{peters2014comparison}.

\subsection{Sensors and actuators}

Gazebo also allows sensors and actuators that will interact with the world and send data back to ROS packages. Sensors can include cameras, laser scanners, and actuators can include drive modules. The sensors are added in the SDF or URDF files. The example below shows the main guidelines to add a camera.

\begin{lstlisting}
  <gazebo reference="camera_link">
    <sensor type="camera" name="camera1">
      <update_rate>30.0</update_rate>
      <camera name="head">
        <horizontal_fov>1.3962634</horizontal_fov>
        <image>
          <width>800</width>
          <height>800</height>
          <format>R8G8B8</format>
        </image>
        ...
      </camera>
      <plugin name="camera_controller" filename="libgazebo_ros_camera.so">
        <alwaysOn>true</alwaysOn>
        <updateRate>0.0</updateRate>
        <cameraName>robot/cam1</cameraName>
        <imageTopicName>image_raw</imageTopicName>
        <cameraInfoTopicName>camera_info</cameraInfoTopicName>
        <frameName>camera_link</frameName>
        <hackBaseline>0.07</hackBaseline>
        <distortionK1>0.0</distortionK1>
        <distortionK2>0.0</distortionK2>
        <distortionK3>0.0</distortionK3>
        <distortionT1>0.0</distortionT1>
        <distortionT2>0.0</distortionT2>
      </plugin>
    </sensor>
  </gazebo>
\end{lstlisting}

Notice that it must include values for image size and update rate on the sensor type. The controller is loaded from a plugin called \texttt{libgazebo\_ros\_camera.so}, that uses as parameters which topic the camera will correspond to and the intrinsic parameters of the lenses, as well as the link the camera will be attached to.

There are many pre-built plugins available at Gazebo library, including Cameras (mono and stereo), Kinect, Laser Scanner, Force and IMU sensors, Differential Drive, Skid Steering Drive, as well as templates to write your own dedicated plugin.

%\section{OpenCV}

%\section{MoveIT!}

\section{COB}

The Care-o-bot is a project for a mobile assistive robot that is modular, developed and maintained by Fraunhofer IPA. The COB version 4 can be seen on \prettyref{fig:cob4}. The robot was composed not only to provide researchers a reliable mobile base, but also to aid research on human-robot interaction and social behavior \cite{mci/Kittmann2015}. It is composed mainly by a mobile base, a torso and a head.

% TODO: editar a imagem para conter descrição de partes
\begin{figure}[!ht]
\centering
\includegraphics[width=.8\linewidth]{cob4}
\caption{COB 4 full robot with both manipulators.}
\label{fig:cob4}
\end{figure}

\subsection{Base}

The base features three steerable wheels used for moving the robot on the ground. The robot has three wheels so that it can move freely on the ground. Because the modularity, the wheels can be configured to behave using Ackermann kinematics, moving forwards and backwards and rotating on the vertival axis, but also Omnidirectional kinematics, allowing the robot to move in every direction at a maximum speed of 1,1 m/s. It is also equipped with 2D laser scanners with 360$^{\circ}$ coverage for object detection and safety, and the battery for the robot and the control panel.

\subsection{Torso}

The torso is linked with the base, and can be configured to be fixed (the torso doesn't move), to use a Pan joint (allowing for a full rotation) or using a spherical joint (providing 3DOF). It can have two optional arms with 7DOF, each with a manipulator finger with 2DOF. The arm is composed by three SCHUNK Powerball ERB modules and one PRL+ 100 module  in  the  shoulder. The manipulator was developed by Schunk especifically for this robot.

The manipulator also has a 3D camera and laser scanner for object recognition and picking, as well as LEDs for illumination. The torso also provides 3D cameras and sensors for computer vision activities that cover up the front of the robot, a LED Ring for signaling and an emergency stop.

\subsection{Head}

The head is linked with torso, allowing both a pan joint and a spherical joint, and contain the human interface to interact with user, including the sound system, microphone, a touch screen display and optional camera for face recognition.


\subsection{Package organization}

The COB is built around ROS and includes a different sets of packages for different purposes. Since the robot support different configurations (manipulators, joints, mobile bases), some packages are optional. \prettyref{fig:dependency-graph} shows the dependency graph for the first three layers of packages. Notice that the arrangement is quite intricate even showing only the packages written exclusively for COB (not showing other ROS packages used on COB).

\begin{figure}[!ht]
\centering
\includegraphics[width=.7\linewidth]{dependency-graph}
\caption{Dependency graph of \texttt{cob\_simulation} with depth 3.}
\label{fig:dependency-graph}
\end{figure}


The COB core consist of the following packages and its dependencies:

\begin{itemize}
\item \texttt{cob\_msgs}: Robot-specific Messages, representing state information like battery status, etc.
\item \texttt{cob\_srvs}: Robot-specific Services.
\item \texttt{cob\_description}: Robot URDF models different COB configurations (only base, base with fixed torso, base with actuated torso, etc).
\item \texttt{cob\_bringup}: machine configuration, including all scripts and dependencies required to run COB.
\end{itemize}

COB also features high-level capabilities, some of them being:

\begin{itemize}
\item \texttt{cob\_command\_tools}: high-level utilities command tools, including API for commonly used movements, control dashboard, teleoperation and status monitoring.
\item \texttt{cob\_driver}: plugins interfacing motors, LEDs, sound system, cameras, batteries and even the facial expressions for the robot, and providing their data in the form of topics and services.
\item \texttt{cob\_navigation}: provides tools for robot navigation, including creation of maps, navigation with/without collision avoidance and navigation in dynamic environments.
\item \texttt{cob\_object\_perception}, \texttt{cob\_people\_perception}, \texttt{cob\_environment\_perception}: computer vision libraries used for perception of the environment.
\item \texttt{cob\_manipulation}: manipulator related package, including inverse kinematics, arm motion planning and collision avoidance.
\end{itemize}

\subsection{Basic API}

Most of the capabilities of COB are exposed in the form of topics and services, forming an higher level API and sparing the user from interacting with low level sensors and actuators.

The actuators to move the three wheels base. Since the base kinematics is already calculated by the node, you only need to supply a \texttt{geometry\_msgs/Twist} type message with linear and angular velocities and the drivers will take care of the rest. The topic names can be seen on \prettyref{tab:baseapi}.

\begin{table}[!ht]
\caption{Base command API.} \label{tab:baseapi}
\renewcommand*{\arraystretch}{1.1}
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|c}
Topic Name & Message Type & Information \\
\hline
\makecell{\texttt{/base/twist\_mux} \\ \texttt{/command\_navigation}} & \texttt{geometry\_msgs/Twist} & Velocity topics related to navigation \\ \hline
\makecell{\texttt{/base/twist\_mux} \\ \texttt{/command\_safe}} & \texttt{geometry\_msgs/Twist} & \makecell{Velocity topics related to teleoperation \\ with collision checking and smoothing} \\ \hline
\makecell{\texttt{/base/twist\_controller} \\ \texttt{/command}} & \texttt{geometry\_msgs/Twist} & \makecell{Low level Velocity topics \\ for control purposes} \\ \hline
\end{tabular}
}
\end{table}

Since the torso and the head only have one joint, they receive a set of points forming a trajectory to follow, given by the \texttt{control\_msgs/FollowJointTrajectory} service message type. This service returns if the robot was able to perform the trajectory, as well as the error in the path. The structure can be seen on \prettyref{tab:headapi}.

\begin{table}[!ht]
\caption{Torso and head command API.} \label{tab:headapi}
\renewcommand*{\arraystretch}{1.1}
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|c}
Topic Name & Message Type & Information \\
\hline
\makecell{\texttt{/torso/joint\_trajectory\_controller} \\ \texttt{/follow\_joint\_trajectory}} & \texttt{control\_msgs/FollowJointTrajectory} & Trajectory to move the torso \\ \hline
\makecell{\texttt{/head/joint\_trajectory\_controller} \\ \texttt{/follow\_joint\_trajectory}} & \texttt{control\_msgs/FollowJointTrajectory} & Trajectory to move the head \\ \hline
\end{tabular}
}
\end{table}

The Arms, the Grippers and the sensor ring are similar to the torso and head, as they act as a service and receive the same message type. Their API is shown on \prettyref{tab:armapi}.

\begin{table}[!ht]
\caption{Arms and grippers command API.} \label{tab:armapi}
\renewcommand*{\arraystretch}{1.1}
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|c}
Topic Name & Message Type & Information \\
\hline
\makecell{\texttt{/arm\_left/joint\_trajectory\_controller} \\ \texttt{/follow\_joint\_trajectory}} & \texttt{control\_msgs/FollowJointTrajectory} & Trajectory to move the left arm \\ \hline
\makecell{\texttt{/gripper\_left} \\ \texttt{/joint\_trajectory\_controller} \\ \texttt{/follow\_joint\_trajectory}} & \texttt{control\_msgs/FollowJointTrajectory} & Trajectory to move the left gripper \\ \hline
\makecell{\texttt{/arm\_right/joint\_trajectory\_controller} \\ \texttt{/follow\_joint\_trajectory}} & \texttt{control\_msgs/FollowJointTrajectory} & Trajectory to move the right arm \\ \hline
\makecell{\texttt{/gripper\_right} \\ \texttt{/joint\_trajectory\_controller} \\ \texttt{/follow\_joint\_trajectory}} & \texttt{control\_msgs/FollowJointTrajectory} & Trajectory to move the right gripper \\ \hline
\makecell{\texttt{/sensorring} \\ \texttt{/joint\_trajectory\_controller} \\ \texttt{/follow\_joint\_trajectory}} & \texttt{control\_msgs/FollowJointTrajectory} & Trajectory to move the sensor ring ring\\ \hline
\end{tabular}
}
\end{table}

For the sensors, there are three lasers scans that cover the entire circumference of the robot, shown on \prettyref{tab:laserapi}. Even though they are separate entities, there are nodes that transform the three different measurements into a single one for easier use later on.

\begin{table}[!ht]
\caption{Laser Scan API.} \label{tab:laserapi}
\centering
\renewcommand*{\arraystretch}{1.1}
\begin{tabular}{c|c|c}
Topic Name & Message Type & Information \\
\hline
\texttt{/base\_laser\_front/scan} & \texttt{sensor\_msgs/LaserScan} & Front laser scan \\ \hline
\texttt{/base\_laser\_left/scan} & \texttt{sensor\_msgs/LaserScan} & Left laser scan \\ \hline
\texttt{/base\_laser\_right/scan} & \texttt{sensor\_msgs/LaserScan} & Right laser scan \\ \hline
\end{tabular}
\end{table}

There are also a cameras in the torso, head and in the sensor ring that collect both raw images and a Point Cloud representation that includes the distance of each point to the focal point of the camera. Their respective topics can be seen on \prettyref{tab:cameraapi}.

\begin{table}[!ht]
\caption{Cameras API.} \label{tab:cameraapi}
\renewcommand*{\arraystretch}{1.1}
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|c}
Topic Name & Message Type & Information \\
\hline
\texttt{/torso\_cam3d\_left/rgb/image\_raw} & \texttt{sensor\_msgs/Image} & \makecell{Color image of the \\ left torso camera} \\ \hline
\texttt{/torso\_cam3d\_left/depth\_registered/points} & \texttt{sensor\_msgs/PointCloud2} & \makecell{Depth data from \\ torso left camera} \\ \hline % left camera
\texttt{/torso\_cam3d\_right/rgb/image\_raw} & \texttt{sensor\_msgs/Image} & \makecell{Color image of the \\ right torso camera} \\ \hline
\texttt{/torso\_cam3d\_right/depth\_registered/points} & \texttt{sensor\_msgs/PointCloud2} & \makecell{Depth data from \\ torso right camera} \\ \hline % right camera
\texttt{/torso\_cam3d\_right/rgb/image\_raw} & \texttt{sensor\_msgs/Image} & \makecell{Color image of the \\ down torso camera} \\ \hline
\texttt{/torso\_cam3d\_down/depth\_registered/points} & \texttt{sensor\_msgs/PointCloud2} & \makecell{Depth data from \\ torso down camera} \\ \hline % down camera
\texttt{/sensorring\_cam3d\_front/depth/points} & \texttt{sensor\_msgs/PointCloud2} & \makecell{Depth data from \\ sensor ring camera} \\ \hline
\texttt{/sensorring\_cam3d\_back/rgb/image\_raw} & \texttt{sensor\_msgs/Image} & \makecell{Color image of the \\ back sensor ring camera} \\ \hline
\texttt{/torso\_cam3d\_down/depth\_registered/points} & \texttt{sensor\_msgs/PointCloud2} & \makecell{Depth data from \\ back sensor ring camera} \\ \hline % sensor ring back camera
\texttt{/head\_cam3d/rgb/image\_raw} & \texttt{sensor\_msgs/Image} & \makecell{Color image of the \\ head camera} \\ \hline % head camera
\end{tabular}
}
\end{table}

Finally, there are also other miscellaneous topics to control the lights and text-to-speak output and publish robot information, shown on \prettyref{tab:miscapi}.

\begin{table}[!ht]
\caption{Miscellaneous API.} \label{tab:miscapi}
\centering
\renewcommand*{\arraystretch}{1.1}
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|c}
Topic Name & Message Type & Information \\
\hline
\texttt{/joy} & \texttt{sensor\_msgs/Joy} & Input commands of joystick \\ \hline
\texttt{/sound/say} & \texttt{cob\_sound/Say} & Text for text-to-speak output \\ \hline
\texttt{/light\_base/set\_light} & \texttt{cob\_light/SetLightMode} & Command for base lights \\ \hline
\texttt{/light\_torso/set\_light} & \texttt{cob\_light/SetLightMode} & Command for torso lights \\ \hline
\texttt{/light\_torso/set\_light} & \texttt{cob\_light/SetLightMode} & Command for torso lights \\ \hline
\texttt{/emergency\_stop\_state} & \texttt{cob\_msgs/EmergencyStopState} & \makecell{Laser and button \\ stop information.} \\ \hline
\texttt{/power\_state} & \texttt{cob\_msgs/PowerState} & \makecell{Battery information.} \\ \hline
\end{tabular}
}
\end{table}